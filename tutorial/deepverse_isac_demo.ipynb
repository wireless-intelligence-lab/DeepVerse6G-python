{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WCqTrkL5s3l"
      },
      "source": [
        "# DeepVerse-Python ISAC Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rs2W9Ah-2Czp"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U9pQhC1vHdH"
      },
      "source": [
        "In this notebook, we present an ISAC demo where the radar signal at the base station is used to predict the optimal communication beam for the BS to serve the mobile user (vehicle), using a deep learning model.\n",
        "- Data generation: How to generate radar and communication data using DeepVerse-Python\n",
        "- Data postprocessing: How to prepare the training and test datasets\n",
        "- Radar-aided beam prediction: Deep learning model training and testing\n",
        "\n",
        "\n",
        "See more detailed [API documentation](https://deepverse6g.net/documentation)\\\n",
        "See more detailed [DeepVerse generator tutorial](https://deepverse6g.net/tutorials)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Installation needed starting from a fresh conda env\n",
        "# # This may not produce a torch installation supporting cuda and gpu\n",
        "# %pip install torch numpy pandas tqdm scikit-learn requests deepverse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import zipfile\n",
        "import requests\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from deepverse import ParameterManager, Dataset # pip install deepverse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download DeepVerse data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we download the DeepVerse data and prepare the scenario folder. We use the DT1 scenario in this demo. The DT1 incoporates the LiDAR, RGB images, wireless, and position data modalities. Since this demo focuses on the ISAC application, we only download and use the wireless data modality.\n",
        "\n",
        "The scenario folder follows the below structure\n",
        "```\n",
        "DeepVerse-main/\n",
        "├─ scenarios/\n",
        "│  ├─ DT1/\n",
        "│  │  ├─ wireless/\n",
        "│  │  │  ├─ ...\n",
        "│  │  │  ├─ params.mat\n",
        "│  │  ├─ param/\n",
        "│  │  │  ├─ config.m\n",
        "│  │  ├─ scenario1.csv\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbdF-W_15n1V",
        "outputId": "f784aa4d-6ee5-4d18-a49a-cea570c49636"
      },
      "outputs": [],
      "source": [
        "# Set up directories\n",
        "scenario_name = 'DT1'\n",
        "scenario_dir = Path(f\"scenarios/{scenario_name}\")\n",
        "scenario_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def download_and_unzip(url, zip_path, extract_to):\n",
        "    response = requests.get(url, stream=True)\n",
        "    response.raise_for_status()\n",
        "    \n",
        "    total = int(response.headers.get('content-length', 0))\n",
        "    with open(zip_path, 'wb') as f, tqdm(\n",
        "        desc=f\"Downloading: {zip_path.name}\",\n",
        "        total=total,\n",
        "        unit='B',\n",
        "        unit_scale=True,\n",
        "        unit_divisor=1024,\n",
        "    ) as bar:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            size = f.write(chunk)\n",
        "            bar.update(size)\n",
        "\n",
        "    print(f\"Infalting: {zip_path}\")\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_to)\n",
        "    except zipfile.BadZipFile:\n",
        "        print(f\"Error: {zip_path} is not a valid zip file!\")\n",
        "        return\n",
        "\n",
        "    print(f\"Removing zip: {zip_path}\")\n",
        "    zip_path.unlink()\n",
        "\n",
        "# Download and extract wireless data\n",
        "print(\"Preparing wireless data\")\n",
        "download_and_unzip(\n",
        "    \"https://www.dropbox.com/scl/fi/pyzmcb5jv3oo9e29nymw8/wireless.zip?rlkey=b5gbkgd8dng6hpf91rv7za9ix&e=1&st=5opqw9lc&dl=1\",\n",
        "    scenario_dir / \"wireless.zip\",\n",
        "    scenario_dir\n",
        ")\n",
        "\n",
        "# Download and extract parameter files\n",
        "print(\"Preparing param files\")\n",
        "param_dir = scenario_dir / \"param\"\n",
        "param_dir.mkdir(exist_ok=True)\n",
        "download_and_unzip(\n",
        "    \"https://www.dropbox.com/scl/fo/9sfd6u8912l7o407fqi30/AN2NIxPUrXvMEVjImsHmX2g?rlkey=qqxzkohhnmgjgz2abf6cvb32h&st=0kbpp7v4&dl=1\",\n",
        "    scenario_dir / \"param.zip\",\n",
        "    param_dir\n",
        ")\n",
        "\n",
        "# Copy wireless params.mat file to wireless folder\n",
        "wireless_dir = scenario_dir / \"wireless\"\n",
        "shutil.copy(param_dir / \"params.mat\", wireless_dir / \"params.mat\")\n",
        "shutil.copy(param_dir / \"scenario1.csv\", scenario_dir / \"scenario1.csv\")\n",
        "\n",
        "print('Completed')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DeepVerse dataset generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# path to the configuration file\n",
        "config_path = f'scenarios/{scenario_name}/param/config.m'\n",
        "\n",
        "# initialize ParameterManager and load parameters\n",
        "param_manager = ParameterManager(config_path)\n",
        "param_manager.params[\"scenes\"] = list(range(10)) # 2411\n",
        "param_manager.params[\"radar\"][\"enable\"] = True\n",
        "param_manager.params[\"comm\"][\"enable\"] = True\n",
        "param_manager.params[\"camera\"] = False\n",
        "param_manager.params[\"lidar\"] = False\n",
        "param_manager.params[\"position\"] = False\n",
        "\n",
        "# generate a dataset\n",
        "dataset = Dataset(param_manager)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data postprcessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The generated DeepVerse dataset consists of:\n",
        "- The communication channel between the basestation and the user\n",
        "- The radar intermediate frequency (IF) signal at the base station\n",
        "\n",
        "In the following blocks we apply postprocessing on this data:\n",
        "- We apply beamforming (from a beam codebook) to the communication channel to get the communication beam power\n",
        "- We process the radar IF signal to generate the range-angle map\n",
        "\n",
        "We will then save the postprocessed data.\n",
        "\n",
        "Following the postprocessing, we divide the full dataset to training and testing datasets.\n",
        "Additionaly, we extract the index of the optimal communication beam (i.e. highest power).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simple beam-steering codebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def construct_codebook():\n",
        "    def beam_steering_codebook(angles, num_z, num_x):\n",
        "        d = 0.5\n",
        "        k_z = np.arange(num_z)\n",
        "        k_x = np.arange(num_x)\n",
        "        \n",
        "        codebook = []\n",
        "        \n",
        "        for beam_idx in range(angles.shape[0]):\n",
        "            z_angle = angles[beam_idx, 0]\n",
        "            x_angle = angles[beam_idx, 1]\n",
        "            bf_vector_z = np.exp(1j * 2 * np.pi * k_z * d * np.cos(np.radians(z_angle)))\n",
        "            bf_vector_x = np.exp(1j * 2 * np.pi * k_x * d * np.cos(np.radians(x_angle)))\n",
        "            bf_vector = np.outer(bf_vector_z, bf_vector_x).flatten()\n",
        "            codebook.append(bf_vector)\n",
        "        \n",
        "        return np.stack(codebook, axis=0)\n",
        "    # construct beam steering codebook\n",
        "    num_angles = 64\n",
        "    x_angles = np.linspace(0, 180, num_angles + 1)[1:]\n",
        "    x_angles = np.flip(x_angles)\n",
        "    z_angles = np.full(num_angles, 90)\n",
        "    beam_angles = np.column_stack((z_angles, x_angles))\n",
        "    return beam_steering_codebook(beam_angles, 1, 16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Radar range-anlge map processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def range_angle_map(data, fft_size = 64):\n",
        "    data = torch.from_numpy(data)\n",
        "    data = torch.fft.fft(data, axis=1) # Range FFT\n",
        "    data -= torch.mean(data, axis=2, keepdims=True) # Static removal\n",
        "    data = torch.fft.fft(data, n=fft_size, axis=0) # Angle FFT\n",
        "    data = torch.abs(data).sum(axis=2, keepdim=True) # Sum over velocity\n",
        "    data = data.numpy().transpose([2,1,0])\n",
        "    return data*100."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save communication beam power and radar range-angle map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# apply codebook to bs-ue comm channel\n",
        "codebook = construct_codebook()\n",
        "beam_power = []\n",
        "num_scene = len(dataset.params['scenes'])\n",
        "for i in range(num_scene):\n",
        "    channel = dataset.get_sample('comm-ue', index=i, bs_idx=0, ue_idx=0).coeffs\n",
        "    beam_power_ = (np.abs(codebook @ np.squeeze(channel, 0))**2).sum(-1)\n",
        "    beam_power.append(beam_power_)\n",
        "\n",
        "# save data\n",
        "os.makedirs(f'scenarios/{scenario_name}/radar', exist_ok=True)\n",
        "os.makedirs(f'scenarios/{scenario_name}/power', exist_ok=True)\n",
        "for i in range(num_scene):\n",
        "    # get beam power\n",
        "    ue_channel = dataset.get_sample('comm-ue', index=i, bs_idx=0, ue_idx=0).coeffs\n",
        "    beam_power = (np.abs(codebook @ np.squeeze(ue_channel, 0))**2).sum(-1)\n",
        "    np.savetxt(f'scenarios/{scenario_name}/power/{i}.txt', beam_power)\n",
        "\n",
        "    # get radar IF\n",
        "    radar_IF = dataset.get_sample('radar', index=i, bs_idx=0, ue_idx=0).coeffs.astype(np.complex64).squeeze(1)\n",
        "    radar_ra = range_angle_map(radar_IF, fft_size=64)\n",
        "    np.save(f'scenarios/{scenario_name}/radar/{i}.npy', radar_ra)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create training and testing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "csv_path = Path(f\"scenarios/{scenario_name}/scenario1.csv\")\n",
        "csv_dir = csv_path.parent\n",
        "\n",
        "# Load the CSV with index and seq_index\n",
        "df = pd.read_csv(csv_path)  # Replace with your actual file path\n",
        "\n",
        "# Keep only index and seq_index, and rename index -> scene\n",
        "df = df[['index', 'seq_index']].rename(columns={'index': 'scene'})\n",
        "\n",
        "# Generate the power and radar columns\n",
        "df['power'] = df.index.map(lambda i: f'./power/{i}.txt')\n",
        "df['radar'] = df.index.map(lambda i: f'./radar/{i}.npy')\n",
        "\n",
        "# Save or inspect\n",
        "df.to_csv(csv_dir / f'{scenario_name}.csv', index=False)\n",
        "\n",
        "# Compute beam_index from power file\n",
        "def get_beam_index(rel_path):\n",
        "    try:\n",
        "        abs_path = csv_dir / rel_path.strip()\n",
        "        arr = np.loadtxt(abs_path)\n",
        "        return int(np.argmax(arr))\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {rel_path}: {e}\")\n",
        "        return -1\n",
        "\n",
        "df['beam'] = df['power'].apply(get_beam_index)\n",
        "\n",
        "# Split without data leakage based on seq_index\n",
        "unique_seq_indices = df['seq_index'].unique()\n",
        "train_seqs, test_seqs = train_test_split(unique_seq_indices, test_size=0.2, random_state=42)\n",
        "\n",
        "train_df = df[df['seq_index'].isin(train_seqs)]\n",
        "test_df = df[df['seq_index'].isin(test_seqs)]\n",
        "\n",
        "# Save the split CSVs\n",
        "train_df.to_csv(csv_dir / f'{scenario_name}_train.csv', index=False)\n",
        "test_df.to_csv(csv_dir / f'{scenario_name}_test.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deep learning demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this block we train a neural network for radar-aided beam prediction\n",
        "- The input of the neural network is the radar range-angle map\n",
        "- The ground-truth output of the neural network is a one-hot vector for the optimal beam index\n",
        "- The loss function for training is the cross-entropy\n",
        "- We evaluate the performance of the beam prediction with top-k accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deep learning model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LeNet_RangeAngle(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet_RangeAngle, self).__init__()\n",
        "        self.pool = nn.AvgPool2d((2, 2))\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(1, 8, 3, padding='same')\n",
        "        self.conv2 = nn.Conv2d(8, 16, 3, padding='same')\n",
        "        self.conv3 = nn.Conv2d(16, 8, 3, padding='same')\n",
        "        self.conv4 = nn.Conv2d(8, 4, 3, padding='same')\n",
        "        self.conv5 = nn.Conv2d(4, 2, 3, padding='same')\n",
        "        self.fc1 = nn.Linear(1024, 4*64)\n",
        "        self.fc2 = nn.Linear(4*64, 2*64)\n",
        "        self.fc3 = nn.Linear(2*64, 64)\n",
        "        self.name = 'LeNet_RangeAngle'\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = F.relu((self.conv1(x)))\n",
        "        x = F.relu((self.conv2(x)))\n",
        "        x = F.relu(self.pool(self.conv3(x)))\n",
        "        x = F.relu(self.pool(self.conv4(x)))\n",
        "        x = F.relu(self.pool(self.conv5(x)))\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_dataset(root_dir, csv_file, radar_column='radar', label_column='beam'):\n",
        "    csv_file = pd.read_csv(os.path.join(root_dir, csv_file))\n",
        "    X = np.load(os.path.abspath(root_dir +  csv_file[radar_column][0]), allow_pickle=True)\n",
        "    X = np.zeros((len(csv_file),) + X.shape, dtype=X.dtype)\n",
        "    for i in tqdm(range(len(csv_file))):\n",
        "        X[i] = np.load(os.path.abspath(root_dir +  csv_file[radar_column][i]), allow_pickle=True)\n",
        "    y = np.array(csv_file[label_column])\n",
        "    return X, y\n",
        "\n",
        "def train_loop(X_train, y_train, net, optimizer, criterion, device, batch_size=64):\n",
        "    net.train()\n",
        "        \n",
        "    running_acc = 0.0\n",
        "    running_loss = 0.0\n",
        "    running_size = 0\n",
        "    with tqdm(iterate_minibatches(X_train, y_train, batch_size, shuffle=True), unit=' batch', \n",
        "              total=int(np.ceil(X_train.shape[0]/batch_size)), file=sys.stdout, leave=True) as tepoch:\n",
        "        for batch_x, batch_y in tepoch:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "            \n",
        "            optimizer.zero_grad() # Make the gradients zero\n",
        "            batch_y_hat = net(batch_x) # Prediction\n",
        "            loss = criterion(batch_y_hat, batch_y) # Loss computation\n",
        "            loss.backward() # Backward step\n",
        "            optimizer.step() # Update coefficients\n",
        "            \n",
        "            predictions = batch_y_hat.argmax(dim=1, keepdim=True).squeeze()\n",
        "            \n",
        "            running_acc += (predictions == batch_y).sum().item()\n",
        "            running_loss += loss.item() * batch_x.shape[0]\n",
        "            running_size += batch_x.shape[0]\n",
        "            curr_acc = 100. * running_acc/running_size\n",
        "            curr_loss = running_loss/running_size\n",
        "\n",
        "            \n",
        "            tepoch.set_postfix(loss=curr_loss, accuracy=curr_acc)\n",
        "            \n",
        "        curr_acc = 100. * running_acc/len(X_train)\n",
        "        curr_loss = running_loss/np.ceil(X_train.shape[0]/batch_size)\n",
        "\n",
        "    return curr_loss, curr_acc\n",
        "    \n",
        "def eval_loop(X_val, y_val, net, criterion, device, batch_size=64):\n",
        "    net.eval()\n",
        "    \n",
        "    running_acc = 0.0\n",
        "    running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in iterate_minibatches(X_val, y_val, batch_size, shuffle=False):\n",
        "            \n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "            \n",
        "            batch_y_hat = net(batch_x) # Prediction\n",
        "    \n",
        "            predictions = batch_y_hat.argmax(dim=1, keepdim=True).squeeze()\n",
        "            running_acc += (predictions == batch_y).sum().item()\n",
        "            running_loss += criterion(batch_y_hat, batch_y).item() * batch_x.shape[0]\n",
        "        \n",
        "    \n",
        "    curr_acc = 100. * running_acc/len(X_val)\n",
        "    curr_loss = running_loss/len(X_val)\n",
        "    print('Validation: [accuracy=%.2f, loss=%.4f]' % (curr_acc, curr_loss), flush=True)\n",
        "    \n",
        "    return curr_loss, curr_acc\n",
        "    \n",
        "def test_loop(X_test, y_test, net, device, model_path=None):\n",
        "    # Network Setup\n",
        "    if model_path is not None:\n",
        "        net.load_state_dict(torch.load(model_path))\n",
        "    net.eval()\n",
        "\n",
        "    data_shape_single = list(X_test.shape)\n",
        "    data_shape_single[0] = 1\n",
        "    data_shape_single = tuple(data_shape_single)\n",
        "    dummy_input = torch.randn(data_shape_single, dtype=torch.float, device=device)\n",
        "    out = net(dummy_input)\n",
        "    \n",
        "    y = -1*torch.ones(len(X_test))\n",
        "    y_hat = -1*torch.ones((len(X_test), out.shape[1])) #Top 5\n",
        "    \n",
        "    cur_ind = 0\n",
        "    \n",
        "    # Test\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in iterate_minibatches(X_test, y_test, batchsize=1, shuffle=False):\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "            batch_y_hat = net(batch_x) # Prediction\n",
        "\n",
        "            next_ind = cur_ind + batch_x.shape[0]\n",
        "            y[cur_ind:next_ind] = batch_y\n",
        "            y_hat[cur_ind:next_ind, :] = batch_y_hat\n",
        "            cur_ind = next_ind\n",
        "\n",
        "    \n",
        "    return y, y_hat\n",
        "    \n",
        "\n",
        "def evaluate_predictions(y, y_hat, k):\n",
        "    topk_pred = torch.topk(y_hat, k=k).indices\n",
        "    topk_acc = np.zeros(k)\n",
        "    for i in range(k):\n",
        "        topk_acc[i] = torch.mean((y == topk_pred[:, i])*1.0)\n",
        "    topk_acc = np.cumsum(topk_acc)\n",
        "    \n",
        "    beam_dist = torch.mean(torch.abs(y - topk_pred[:, 0]))\n",
        "    \n",
        "    return topk_acc, beam_dist\n",
        "\n",
        "def iterate_minibatches(X, y, batchsize, shuffle=False):\n",
        "    \n",
        "    data_len = X.shape[0]\n",
        "    indices = np.arange(data_len)\n",
        "    if shuffle:\n",
        "        np.random.shuffle(indices)\n",
        "        \n",
        "    for start_idx in range(0, data_len, batchsize):\n",
        "        end_idx = min(start_idx + batchsize, data_len)\n",
        "        excerpt = indices[start_idx:end_idx]\n",
        "        yield X[excerpt], y[excerpt]     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#############################\n",
        "######## Load dataset #######\n",
        "#############################\n",
        "\n",
        "# dataset files\n",
        "data_root_dir = f'scenarios/{scenario_name}'\n",
        "train_csv = f'{scenario_name}_train.csv'\n",
        "test_csv = f'{scenario_name}_test.csv'\n",
        "\n",
        "# load and preprocess data\n",
        "print('Preparing training dataset')\n",
        "X_train, y_train = load_dataset(data_root_dir,train_csv)\n",
        "print('Preparing test dataset')\n",
        "X_test, y_test = load_dataset(data_root_dir, test_csv)\n",
        "X_train = torch.from_numpy(X_train)\n",
        "X_test = torch.from_numpy(X_test)\n",
        "y_train = torch.from_numpy(y_train)\n",
        "y_test = torch.from_numpy(y_test)\n",
        "\n",
        "\n",
        "##########################\n",
        "######## Training ########\n",
        "##########################\n",
        "\n",
        "# training hyperparameters\n",
        "batch_size = 32\n",
        "num_epoch = 40\n",
        "learning_rate = 1e-3\n",
        "\n",
        "# ckpts save dir\n",
        "ckpt_dir = os.path.abspath(f'./checkpoints')\n",
        "if not os.path.exists(ckpt_dir):\n",
        "    os.makedirs(ckpt_dir)\n",
        "print('Model ckpt path: %s' % ckpt_dir)\n",
        "\n",
        "# reproducibility\n",
        "torch.manual_seed(1115)\n",
        "np.random.seed(1115)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# pytorch device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# init model\n",
        "net = LeNet_RangeAngle().to(device)\n",
        "\n",
        "# optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "# epochs\n",
        "print('Training')\n",
        "for epoch in range(num_epoch):\n",
        "    print('Epoch %i/%i:'%(epoch+1, num_epoch), flush=True)\n",
        "    train_loop(X_train, y_train, net, optimizer, criterion, device, batch_size=batch_size)\n",
        "    eval_loop(X_test, y_test, net, criterion, device, batch_size=batch_size)\n",
        "    scheduler.step()\n",
        "\n",
        "torch.save(net.state_dict(), os.path.join(ckpt_dir, f'{net.name}.pth'))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Testing')\n",
        "topk = 5\n",
        "y_final, y_hat_final = test_loop(X_test, y_test, net, device, model_path=os.path.join(ckpt_dir, f'{net.name}.pth'))\n",
        "topk_acc_final, beam_dist_final = evaluate_predictions(y_final, y_hat_final, k=topk)\n",
        "print('Top-k Accuracy: ' + ' - '.join(['%.2f' for _ in range(topk)]) % tuple(topk_acc_final*100))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ultralytics_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
